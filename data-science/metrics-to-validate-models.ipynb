{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to validate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "*Accuracy* is the proportion of correct predictions (both **true positives** and **true negatives**) among the total number of cases examined. The general term *accuracy* is used to describe the closeness of a measurement to the true value.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
    "\n",
    "where $\\text{TP}$ = True positive; $\\text{FP}$ = False positive; $\\text{TN}$ = True negative; $\\text{FN}$ = False negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "The *Mean Squared Error* (*MSE*) of an estimator measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum^{n}_{i=1}{(Y_{i} - \\hat{Y}_{i})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRESS\n",
    "The *PRESS* statistic (Predicted Residual Sum of Squares) is a form of *cross-validation* used in regression analysis to provide a summary measure of the fit of a model to a sample of observations that were not themselves used to estimate the model. It is calculated as the sums of squares of the prediction residuals for those observations. See also *Leave-one-out cross-validation*.\n",
    "\n",
    "A *fitted model* having been produced, each observation in turn is removed and the model is refitted using the remaining observations. The out-of-sample predicted value is calculated for the omitted observation in each case, and the *PRESS* statistic is calculated as the sum of the squares of all the resulting prediction errors:\n",
    "\n",
    "$$ \\text{PRESS} = \\sum_{i=1}^n{(y_i - \\hat{y}_{i, -i})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for linear models\n",
    "While cross-validation can be computationally expensive in general, it is very easy and fast to compute *Leave-one-out* Cross-validation (*LOOCV*) for linear models. A linear model can be written as\n",
    "$$\\textbf{Y} = \\textbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}$$\n",
    "Then\n",
    "$$\\hat{\\mathbf{\\beta}} = (\\textbf{X}^\\text{T}\\textbf{X})^{-1}\\textbf{X}^\\text{T}\\textbf{Y}$$\n",
    "and the fitted values can be calculated using\n",
    "$$\\hat{\\textbf{Y}} = \\textbf{X}\\hat{\\mathbf{\\beta}} = (\\textbf{X}^\\text{T}\\textbf{X})^{-1}\\textbf{X}^\\text{T}\\textbf{Y} = \\textbf{H}\\textbf{Y}$$\n",
    "\n",
    "where $\\textbf{H} = (\\textbf{X}^\\text{T}\\textbf{X})^{-1}\\textbf{X}^\\text{T}$ is known as the *hat-matrix* because it is used to compute \n",
    "$\\hat{\\textbf{Y}}$ (*Y-hat*).\n",
    "\n",
    "If the diagonal values of $\\textbf{H}$ are denoted by $h_1, \\dots, h_n$, then the cross-validation statistic can be computed using\n",
    "\n",
    "$$\\text{CV} = \\frac{1}{n} \\sum_{i=1}^{n} {\\left(\\frac{\\epsilon_i} {1 - h_i}\\right)^2}$$\n",
    "\n",
    "where $\\epsilon_i$ is the residual obtained from fitting the model to all  observations. Thus, it is not necessary to actually fit  separate models when computing the $\\text{CV}$ statistic for linear models. This remarkable result allows cross-validation to be used while only fitting the model once to all available observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike’s Information Criterion\n",
    "*Akaike’s Information Criterion* is defined as $$\\text{AIC} = -2\\log{\\mathcal{L}} +2p$$ \n",
    "\n",
    "where $\\mathcal{L}$ is the maximized likelihood using all available data for estimation and $p$ is the number of free parameters in the model. Asymptotically, minimizing the AIC is equivalent to minimizing the CV value. This is true for any model, not just linear models. It is this property that makes the $\\text{AIC}$ so useful in model selection when the purpose is prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwarz Bayesian Information Criterion\n",
    "A related measure is Schwarz’s *Bayesian Information Criterion*: $$\\text{BIC} = -2\\log{\\mathcal{L}} + p\\log{n}$$ \n",
    "\n",
    "where $n$ is the number of observations used for estimation. Because of the heavier penalty, the model chosen by $\\text{BIC}$ is either the same as that chosen by $\\text{AIC}$, or one with fewer terms. Asymptotically, for linear models minimizing $\\text{BIC}$ is equivalent to *leave-v-out cross-validation* when $v=n[1-1/(\\log{n}-1)]$.\n",
    "\n",
    "Many statisticians like to use $\\text{BIC}$ because it is consistent — if there is a true underlying model, then with enough data the $\\text{BIC}$ will select that model. However, in reality there is rarely if ever a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts (because the parameter estimates may not be accurate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "1. Accuracy: https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "2. MSE: https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "3. PRESS: https://en.wikipedia.org/wiki/PRESS_statistic, https://robjhyndman.com/hyndsight/crossvalidation/\n",
    "4. Information Criteria: https://robjhyndman.com/hyndsight/crossvalidation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
