{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deodorant instant liking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data_train_reduced.csv* has survey results of 5 deodorants with several information. I have listed some of the key ones below.\n",
    "\n",
    "###### Dependent variable\n",
    "\n",
    "Instant Liking\n",
    "\n",
    "###### Independent variables (some of them)\n",
    "\n",
    "Respondent ID\n",
    "\n",
    "Product ID\n",
    "\n",
    "Where the first few words uttered by the user after using the particular for the first time positive or negative\n",
    "\n",
    "Strength of the Deo (On a scale of 1 to 5)\n",
    "\n",
    "Is the Deo addicitive (On a scale of 1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Respondent.ID</th>\n",
       "      <th>Product.ID</th>\n",
       "      <th>Instant.Liking</th>\n",
       "      <th>q1_1.personal.opinion.of.this.Deodorant</th>\n",
       "      <th>q2_all.words</th>\n",
       "      <th>q3_1.strength.of.the.Deodorant</th>\n",
       "      <th>q4_1.artificial.chemical</th>\n",
       "      <th>q4_2.attractive</th>\n",
       "      <th>q4_3.bold</th>\n",
       "      <th>q4_4.boring</th>\n",
       "      <th>...</th>\n",
       "      <th>ValSegb</th>\n",
       "      <th>s7.involved.in.the.selection.of.the.cosmetic.products</th>\n",
       "      <th>s8.ethnic.background</th>\n",
       "      <th>s9.education</th>\n",
       "      <th>s10.income</th>\n",
       "      <th>s11.marital.status</th>\n",
       "      <th>s12.working.status</th>\n",
       "      <th>s13.2</th>\n",
       "      <th>s13a.b.most.often</th>\n",
       "      <th>s13b.bottles.of.Deodorant.do.you.currently.own</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.00000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.00000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8249.500000</td>\n",
       "      <td>460.400000</td>\n",
       "      <td>0.24720</td>\n",
       "      <td>5.129600</td>\n",
       "      <td>1.121200</td>\n",
       "      <td>3.344400</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.71680</td>\n",
       "      <td>3.546000</td>\n",
       "      <td>2.241600</td>\n",
       "      <td>...</td>\n",
       "      <td>3.522400</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.665200</td>\n",
       "      <td>3.199200</td>\n",
       "      <td>4.996000</td>\n",
       "      <td>1.742000</td>\n",
       "      <td>1.898000</td>\n",
       "      <td>0.025296</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>3.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3433.008516</td>\n",
       "      <td>308.412528</td>\n",
       "      <td>0.43147</td>\n",
       "      <td>1.481918</td>\n",
       "      <td>0.934055</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>1.384618</td>\n",
       "      <td>1.27548</td>\n",
       "      <td>1.261951</td>\n",
       "      <td>1.242838</td>\n",
       "      <td>...</td>\n",
       "      <td>1.964241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.036889</td>\n",
       "      <td>0.970307</td>\n",
       "      <td>2.371125</td>\n",
       "      <td>0.726944</td>\n",
       "      <td>1.440426</td>\n",
       "      <td>0.104856</td>\n",
       "      <td>0.192198</td>\n",
       "      <td>1.620082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3800.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5324.750000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8249.500000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9774.250000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14099.000000</td>\n",
       "      <td>974.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Respondent.ID   Product.ID  Instant.Liking  \\\n",
       "count    2500.000000  2500.000000      2500.00000   \n",
       "mean     8249.500000   460.400000         0.24720   \n",
       "std      3433.008516   308.412528         0.43147   \n",
       "min      3800.000000   121.000000         0.00000   \n",
       "25%      5324.750000   230.000000         0.00000   \n",
       "50%      8249.500000   344.000000         0.00000   \n",
       "75%      9774.250000   633.000000         0.00000   \n",
       "max     14099.000000   974.000000         1.00000   \n",
       "\n",
       "       q1_1.personal.opinion.of.this.Deodorant  q2_all.words  \\\n",
       "count                              2500.000000   2500.000000   \n",
       "mean                                  5.129600      1.121200   \n",
       "std                                   1.481918      0.934055   \n",
       "min                                   1.000000      0.000000   \n",
       "25%                                   5.000000      0.000000   \n",
       "50%                                   5.000000      1.000000   \n",
       "75%                                   6.000000      2.000000   \n",
       "max                                   7.000000      5.000000   \n",
       "\n",
       "       q3_1.strength.of.the.Deodorant  q4_1.artificial.chemical  \\\n",
       "count                     2500.000000               2500.000000   \n",
       "mean                         3.344400                  2.500000   \n",
       "std                          0.743243                  1.384618   \n",
       "min                          1.000000                  1.000000   \n",
       "25%                          3.000000                  1.000000   \n",
       "50%                          3.000000                  2.000000   \n",
       "75%                          4.000000                  4.000000   \n",
       "max                          5.000000                  5.000000   \n",
       "\n",
       "       q4_2.attractive    q4_3.bold  q4_4.boring  ...      ValSegb  \\\n",
       "count       2500.00000  2500.000000  2500.000000  ...  2500.000000   \n",
       "mean           3.71680     3.546000     2.241600  ...     3.522400   \n",
       "std            1.27548     1.261951     1.242838  ...     1.964241   \n",
       "min            1.00000     1.000000     1.000000  ...     1.000000   \n",
       "25%            3.00000     3.000000     1.000000  ...     2.000000   \n",
       "50%            4.00000     4.000000     2.000000  ...     3.000000   \n",
       "75%            5.00000     5.000000     3.000000  ...     5.000000   \n",
       "max            5.00000     5.000000     5.000000  ...     7.000000   \n",
       "\n",
       "       s7.involved.in.the.selection.of.the.cosmetic.products  \\\n",
       "count                                             2500.0       \n",
       "mean                                                 4.0       \n",
       "std                                                  0.0       \n",
       "min                                                  4.0       \n",
       "25%                                                  4.0       \n",
       "50%                                                  4.0       \n",
       "75%                                                  4.0       \n",
       "max                                                  4.0       \n",
       "\n",
       "       s8.ethnic.background  s9.education   s10.income  s11.marital.status  \\\n",
       "count           2500.000000   2500.000000  2500.000000         2500.000000   \n",
       "mean               1.665200      3.199200     4.996000            1.742000   \n",
       "std                1.036889      0.970307     2.371125            0.726944   \n",
       "min                1.000000      2.000000     2.000000            1.000000   \n",
       "25%                1.000000      2.000000     3.000000            1.000000   \n",
       "50%                1.000000      3.000000     4.000000            2.000000   \n",
       "75%                2.000000      4.000000     7.000000            2.000000   \n",
       "max                5.000000      7.000000    10.000000            5.000000   \n",
       "\n",
       "       s12.working.status        s13.2  s13a.b.most.often  \\\n",
       "count         2500.000000  2500.000000        2500.000000   \n",
       "mean             1.898000     0.025296           0.038400   \n",
       "std              1.440426     0.104856           0.192198   \n",
       "min              1.000000     0.000000           0.000000   \n",
       "25%              1.000000     0.000000           0.000000   \n",
       "50%              1.000000     0.000000           0.000000   \n",
       "75%              2.000000     0.000000           0.000000   \n",
       "max              7.000000     1.000000           1.000000   \n",
       "\n",
       "       s13b.bottles.of.Deodorant.do.you.currently.own  \n",
       "count                                     2500.000000  \n",
       "mean                                         3.072000  \n",
       "std                                          1.620082  \n",
       "min                                          1.000000  \n",
       "25%                                          2.000000  \n",
       "50%                                          3.000000  \n",
       "75%                                          4.000000  \n",
       "max                                          6.000000  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data_train_reduced.csv')\n",
    "# df_test = pd.read_csv('test_data.csv') # we don't have the ground truth for these observations\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select y as response variable and drop the other categorical variables\n",
    "y = df['Instant.Liking']\n",
    "df.drop(['Instant.Liking'], axis=1, inplace=True)\n",
    "df.drop(['Product', 'Product.ID', 'Respondent.ID'], axis=1, inplace=True)\n",
    "y_regr = df.iloc[:, 0]\n",
    "df.drop(df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NAs\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of transform is an array, I have to keep track of the columns and index names to reset them into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, rows = df.columns, df.index\n",
    "df = pd.DataFrame(imp.transform(df))\n",
    "df.columns, df.index = cols, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped:  ['s7.involved.in.the.selection.of.the.cosmetic.products']\n"
     ]
    }
   ],
   "source": [
    "# Drop the constant columns\n",
    "to_drop = list()\n",
    "for col in df.describe():\n",
    "    if df.describe()[col]['std'] < 1e-5:\n",
    "        to_drop.append(col)\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "print('Dropped: ', to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize df\n",
    "df_stand = (df - np.mean(df)) / np.std(df)\n",
    "\n",
    "# split into test and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_stand, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is skewed and the majority class classifier has $$ACC = 1-\\bar{p}$$ where \n",
    "- $ACC$ is the classifier's accuracy\n",
    "- $\\bar{p} = \\sum_{i=1}^{n}{x_i/n}$ is the sample mean of the target column *instant liking* $x$\n",
    "\n",
    "It means that estimating new points with the majority class (0) is correct 75% of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.iloc[:, range(29, 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy =  0.7535\n"
     ]
    }
   ],
   "source": [
    "acc_train = lr.predict(x_train) == y_train\n",
    "print('train accuracy = ', sum(acc_train) / len(acc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy =  0.758\n"
     ]
    }
   ],
   "source": [
    "acc_test = lr.predict(x_test) == y_test\n",
    "print('test accuracy = ', sum(acc_test) / len(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some linear regression now\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_stand, y_regr, test_size=0.2, random_state=1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 2.26\n",
      "Coefficient of determination: -0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# y_pred_train = lr.predict(x_train)\n",
    "y_pred_test = lr.predict(x_test)\n",
    "# mse_train = mean_squared_error(y_pred_train, y_train)\n",
    "mse_test = mean_squared_error(y_pred_test, y_test)\n",
    "# print(f'MSE Train: {mse_train:.2f}')\n",
    "print(f'MSE Test: {mse_test:.2f}')\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f'Coefficient of determination: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05823805618782529"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual R^2\n",
    "\n",
    "# SStot = np.sum((y_pred_test - np.mean(y_test))^2, axis=1)\n",
    "SStot = np.sum(np.power(y_test - np.mean(y_test), 2))\n",
    "SSres = np.sum(np.power(y_test - y_pred_test, 2))\n",
    "R2 = 1 - SSres/SStot\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your regression line is ***worse than using the mean value***, the $R^2$ value that you calculate will be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x222b03bc550>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 2.26\n",
      "Coefficient of determination: -0.06\n"
     ]
    }
   ],
   "source": [
    "# Polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transformer\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "df_poly = poly.fit_transform(df_stand)\n",
    "\n",
    "# Split training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_poly, y_regr, test_size=0.2, random_state=1)\n",
    "\n",
    "# Fitting the model\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = lr.predict(x_test)\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "print(f'MSE Test: {mse_test:.2f}')\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f'Coefficient of determination: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 2.26\n",
      "Coefficient of determination: -0.06\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_stand, y_regr, test_size=0.2, random_state=1)\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = ridge.predict(x_test)\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "print(f'MSE Test: {mse_test:.2f}')\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f'Coefficient of determination: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(df_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca components shape: (5, 58)\n",
      "x shape (2500, 58)\n",
      "handcrafted shape: (2500, 5)\n",
      "api shape (2500, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('pca components shape:', pca.components_.shape)\n",
    "print('x shape', df_stand.shape)\n",
    "\n",
    "# I need a design matrix with shape (2500, 5)\n",
    "handcrafted = np.dot(df_stand, pca.components_.T)\n",
    "prinComp = pca.transform(df_stand)\n",
    "\n",
    "print('handcrafted shape:', handcrafted.shape)\n",
    "print('api shape', prinComp.shape)\n",
    "abs(pd.DataFrame(handcrafted) - pd.DataFrame(prinComp)) < 1e-5\n",
    "\n",
    "diff = abs(pd.DataFrame(handcrafted) - pd.DataFrame(prinComp)) < 1e-5\n",
    "diff = diff.values.reshape((-1))\n",
    "np.sum(diff) / len(diff)\n",
    "\n",
    "# THEY ARE THE SAME!!! YAY :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.lda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-367-c7a1cfe07ca2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# LDA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_stand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.lda'"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_stand, y, test_size=0.2, random_state=1)\n",
    "\n",
    "lda = LDA()\n",
    "lda.fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "1. **Weak exogeneity** the predictor variables $x$ can be treated as fixed values, rather than not random variables\n",
    "2. **Linear model** the response variable is a linear combination of the parameters (regression coeffients) and the predictor variables: $$y = mx + q$$\n",
    "3. **Constant variance** aka **homoscedasticity** different values of the response have the same variance in their errors, regardless of the values of the predictor variables.\n",
    "4. **Independence of residuals** the errors of the response variables are independent and uncorrelated with each other.\n",
    "5. **Lack of perfect multicollinearity** there are no variables that are linear combinations of others. The design matrix *X* must have full rank *p*, where *p* is the number of the predictor variables.\n",
    "6. **Normal distribution of the residuals** As sample sizes increase, the normality assumption for the residuals is not needed. More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution. As a consequence, for moderate to large sample sizes, non-normality of residuals should not adversely affect the usual inferential procedures. This result is a consequence of an extremely important result in statistics, known as the *central limit theorem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaxing the assumptions\n",
    "1. Dropping it leads to handle *errors-in-variables* models. \n",
    "2. Considering transformations of the predictor variables do not break this assumption, since the model will be still linear in the interaction between predictor variables and coefficients ($y = mf(x) + q$). But be careful, since models with such as polynomial regression are often \"too powerful\", in that they tend to overfit the data. Adding to the model non-linear terms in the coefficients generate *penalized models* like:\n",
    "    * **ridge regression**, where the sum of squared residuals + $\\lambda \\beta^2$ is minimized. The new model has a slight bigger bias due to the introduced penalty, but a lower variance than the original. If the train dataset is smaller than the test, this tecnique helps to reduce the overfitting, since attributes less weigth to the training data points.\n",
    "    \n",
    "    Another property of ridge regression models is that when you reduce the slope, the model becomes less sensitive to changes in the predictor variables, thus less prone to overfit the data. $\\lambda$ regulates this sensitivity to the predictors: when $\\lambda = 0$ the formula is the basic OLS, with no penalties on the coefficients; when $\\lambda = \\inf$ the model fits a set of coeffiecients close to zero, so the model is totally independent from the predictor variables and predicts every values with $0$.\n",
    "    \n",
    "    The best value for $\\lambda$ is usually found via cross-validation.\n",
    "    \n",
    "    * **lasso regression**, where the sum of squared residuals + $\\lambda|\\beta|$ is minimized. But *lasso is able to set a coefficient at zero*, for the way it is buildt. So can do variable selection as well! It is a little better than ridge regression at reducing the variance in models that contain a lot of predictor variables. \n",
    "    \n",
    "    * **elastic net regression** combines ridge and lasso regression: the regularization term is $\\lambda_{1}\\beta^2 + \\lambda_{2}|\\beta|$. It is especially good at dealing with situations with when there are *correlations between parameters*. Lasso tends to pick just one of the correlated terms and eliminate the other, while ridge regression never sets to zero any parameters and shrinks all the parameters from the correlated variables together. So the correlated variables are all in or out of the final model.\n",
    "\n",
    "\n",
    "3.  In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a \"fanning effect\" between residual error and predicted values. \n",
    "4. idk\n",
    "5. Regularization tecniques can help here in case of *n<p*, i.e. less observations than predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with multicollinearity\n",
    "If two or more predictor variables are dependant, they should change together accordingly. But the linear model assumes lack of multicollinearity, expecting that changing one variable do not influence the other predictor vars. \n",
    "In case of mc, the coefficients lose part of their meaning, since they reflect a change in the response due to a change in *one* predictor variable at a time.\n",
    "\n",
    "The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable *independently* because the independent variables tend to change in unison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with polynomial or interaction terms could be affected by multicollinearity. In order to fix that, you should **standardize** the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity causes the following two basic types of problems:\n",
    "\n",
    "1. The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become **very sensitive** to small changes in the model.\n",
    "2. Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might **not be able to trust the p-values** to identify independent variables that are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you do not always have to fix multicollinearity: the need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
    "\n",
    "1. The severity of the problems **increases with the degree of the multicollinearity**. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "2. Multicollinearity affects only the *specific independent variables that are correlated*. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. A.k.a. if there is multicollinearity in variables you are not interested in, you should not fix it.\n",
    "3. Multicollinearity affects the coefficients and p-values, but ***it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics***. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\n",
    "\n",
    "The last point shall be repeated: if you are only interested in prediction and not in variable selection and/or in undestanding the role of each variable, multicollinearity will not affect:\n",
    "- the predictions\n",
    "- the precision of the predictions\n",
    "- the goodness-of-fit statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *p-values* of coefficients\n",
    "The p-value for each independent/predictor variable tests the null hypothesis that the variable has no correlation with the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Value in Regression\n",
    "The F value in regression is the result of a test where the null hypothesis is that **all of the regression coefficients are equal to zero**. In other words, the model has no predictive capability. Basically, the f-test compares your model with zero predictor variables (the intercept only model), and decides whether your added coefficients improved the model. If you get a significant result, then whatever coefficients you included in your model improved the model’s fit.\n",
    "\n",
    "**Read your p-value first**. If the p-value is small (less than your alpha level), you can reject the null hypothesis. Only then should you consider the f-value. If you don’t reject the null, ignore the f-value.\n",
    "\n",
    "Many authors recommend *ignoring the p-values for individual regression coefficients if the overall F ratio is not statistically significant*. This is because of the multiple testing problem. In other words, your p-value and f-value should both be statistically significant in order to correctly interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating model assumptions\n",
    "1. **Residual analysis**\n",
    "    * **normality distribution of residuals** shapiro wilk test, goodness of fit, \n",
    "    * **heteroscedasticity** inspection of the residuals plot to check \"fan\" or \"bowtie\" effects,\n",
    "\n",
    "2. **Lack of perfect multicollinearity** VIF (variance inflation test), scatteplot to check correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-fit\n",
    "\n",
    "#### $R^2$ - coefficient of determination\n",
    "\n",
    "The proportion of the variance in the response variable that is predictable from the predictor variable(s).\n",
    "\n",
    "*How much variance of the response is explained by the predictor variables.*\n",
    "$$R^2 = \\frac{SS_{Regr}}{SS_{Tot}} = 1 - \\frac{SS_{Res}}{SS_{Tot}}$$\n",
    "Where\n",
    "- $SS_{Regr} = \\sum_{i=1}^{n}(\\hat{y}_{i} - \\bar{y})^2$\n",
    "- $SS_{Res} = \\sum_{i=1}^{n}(\\hat{y}_{i} - y_{i})^2$\n",
    "- $SS_{Tot} = SS_{Regr} + SS_{Res} = \\sum_{i=1}^{n}({y}_{i} - \\bar{y})^2$\n",
    "\n",
    "#### Adjusted $R^2$\n",
    "The use of an adjusted $R^2$ is an attempt to account for the phenomenon of the $R^2$ automatically and spuriously increasing when extra explanatory variables are added to the model. The adjusted $R^2$ is defined as\n",
    "\n",
    "$${\\bar {R}}^{2}=1-(1-R^{2}){n-1 \\over n-p-1}$$\n",
    "where $p$ is the total number of explanatory variables in the model (not including the constant term), and $n$ is the sample size. \n",
    "\n",
    "The adjusted $R^2$ **can be negative**, and its value will **always be less than or equal to that of $R^2$**. Unlike $R^2$, the adjusted $R^2$ increases only when the increase in $R^2$ (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance. \n",
    "\n",
    "Adjusted $R^2$ can be interpreted as an unbiased (or less biased) estimator of the population $R^2$, whereas the observed sample $R^2$ is a positively biased estimate of the population value. Adjusted $R^2$ is more appropriate when evaluating model fit (the variance in the dependent variable accounted for by the independent variables) and in comparing alternative models in the feature selection stage of model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope and intercept\n",
    "###### Interpretation of the slope\n",
    "I represents the *mean change* in the response variable for each 1 unit change in one of the independent variable, holding all other variables constant. \n",
    "\n",
    "###### Interpretation of the intercept\n",
    "Tells the response variable's value whene the independent variable is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality reduction** or **dimension reduction** is the process of *reducing the number of random variables under consideration* by obtaining a set of principal variables. Approaches can be divided into feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA - Principal Components Analysis\n",
    "The main linear technique for dimensionality reduction, principal component analysis, performs a **linear mapping of the data to a lower-dimensional space** in such a way that the variance of the data in the low-dimensional representation is maximized.\n",
    "\n",
    "In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. \n",
    "\n",
    "**The variance explained by each eigenvector is the relative eigevalue**. (WOW)\n",
    "\n",
    "Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. \n",
    "\n",
    "The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA - Linear Discriminant Analysis\n",
    "Linear discriminant analysis (LDA) is a method used to **find a linear combination of features that characterizes or separates two or more classes of objects or events**.\n",
    "\n",
    "\n",
    "LDA is closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data.\n",
    "\n",
    "LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. \n",
    "\n",
    "Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance Matrix\n",
    "In probability theory and statistics, a **covariance matrix** (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance–covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector. In the matrix diagonal there are variances, i.e., the covariance of each element with itself.\n",
    "\n",
    "#### Correlation Matrix\n",
    "The correlation matrix of $n$ random variables $X_{1},\\ldots ,X_{n}$ is the $n\\times n$ matrix whose $(i,j)$ entry is $\\operatorname {corr} (X_{i},X_{j})$.\n",
    "\n",
    "Equivalently, the correlation matrix can be seen as the covariance matrix of the standardized random variables $X_{i}/\\sigma (X_{i})$ for $i=1,\\dots ,n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of dimensionality reduction\n",
    "\n",
    "1. It reduces the time and storage space required.\n",
    "2. Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model.\n",
    "3. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
    "4. It avoids the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Multicollinearity https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
    "2. Residual plots in yellowbrick scikit-yb.org/en/latest/api/regressor/residuals.html\n",
    "3. $R^2$ in scikit-learn https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n",
    "4. Regression, ANOVA & hypothesis testing http://reliawiki.org/index.php/Simple_Linear_Regression_Analysis#:~:text=The%20tests%20are%20used%20to,%2C%20equals%20some%20constant%20value%2C%20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
