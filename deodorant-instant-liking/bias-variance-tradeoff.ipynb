{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics and machine learning, the **bias–variance tradeoff** is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The **bias–variance dilemma** or **bias–variance problem** is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n",
    "\n",
    "1. The *bias error* is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "2. The *variance* is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = f(x) + \\epsilon$\n",
    "\n",
    "$f = f(x) \\text{ is deterministic, thus } \\mathbb{E}[f] = f$\n",
    "\n",
    "$\\mathbb{E}[\\epsilon] = 0 \\implies \\text{Var}[\\epsilon]=\\mathbb{E}[\\epsilon^2] \\text{ because } \\text{Var}[\\epsilon]=\\mathbb{E}[\\epsilon^2]-(\\mathbb{E}[\\epsilon])^2$\n",
    "\n",
    "$\\hat{f} = \\hat{f}(x) \\text{ is the } f \\text{ estimator}$\n",
    "\n",
    "$\\text{MSE} = \\mathbb{E}[(y - \\hat{f})^2]$\n",
    "\n",
    "$\\text{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{aligned}\n",
    " \\text{MSE} = \\mathbb{E}[(y - \\hat{f})^2]  &= \\mathbb{E}[y^2 + \\hat{f}^2 - 2y\\hat{f}] \\\\\n",
    "                              &= \\mathbb{E}[(f + \\epsilon)^2 + \\hat{f}^2 - 2(f + \\epsilon)\\hat{f}] \\\\\n",
    "                              &= \\mathbb{E}[f^2 + \\epsilon^2 + 2f\\epsilon + \\hat{f}^2 - 2f\\hat{f} - \\epsilon\\hat{f}] \\\\\n",
    "                              &= \\mathbb{E}[f^2] + \\mathbb{E}[\\epsilon^2] + \\mathbb{E}[2f\\epsilon] + \\mathbb{E}[\\hat{f}^2] - \\mathbb{E}[2f\\hat{f}] - \\mathbb{E}[\\epsilon\\hat{f}] \\\\\n",
    "                              &= \\{\\mathbb{E}[\\epsilon] = 0\\} \\\\\n",
    "                              &= \\mathbb{E}[f^2] + \\text{Var}[\\epsilon] + \\mathbb{E}[\\hat{f}^2] - \\mathbb{E}[2f\\hat{f}] \\\\\n",
    "                              &= \\mathbb{E}[f^2] + \\text{Var}[\\epsilon] + \\text{Var}[\\hat{f}] + (\\mathbb{E}[\\hat{f}])^2 - \\mathbb{E}[2f\\hat{f}] \\\\\n",
    "                              &= \\mathbb{E}[f^2] - \\mathbb{E}[2f\\hat{f}] + (\\mathbb{E}[\\hat{f}])^2 + \\text{Var}[\\hat{f}] + \\text{Var}[\\epsilon]\\\\\n",
    "                              &= (\\mathbb{E}[f - \\hat{f}])^2 + \\text{Var}[\\hat{f}] + \\text{Var}[\\epsilon]\\\\\n",
    "                              &= (\\text{Bias}[\\hat{f}])^2 + \\text{Var}[\\hat{f}] + \\text{Var}[\\epsilon] \\\\\n",
    "                              &= \\text{Bias}^2 + \\text{Variance} + \\text{Irriducible Error}\n",
    "\\end{aligned}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
